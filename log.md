# 100 Days Of ML Code - Log

### Day 1: July 26, 2018

**Today's Progress**: Housing price prediction problem, spent a lot of time on Data and finally able to train model

**Thoughts**: working on data is really interesting, tried linear regresion, decission tree regressor, random forest with cross validation but still a good model need to find

**Link to work**:[House price prediction](https://github.com/alokpadhi/House-Price-Prediction)

### Day 2: July 27, 2018

**Today's Progress**: Worked on fine tuning the model using Grid search method

**Thoughts**: will use Random search and regularize to couter the overfiting problem

**Link to work**:[House price prediction](https://github.com/alokpadhi/House-Price-Prediction)

### Day 3,July 28, 2018

**Today's Progress**: Tried to learn about different optimization techniques

**Thoughts**: Just a learning phase, using different optimization and to know how and where to use them is really great

### Day 4: July 30, 2018

**Today's Progress**: some data visualization, and testing on different algorithm and finally fine tune them

**Thoughts**: when you're working on a machine learning project it is really necessary to work on data very very well, if your data is good enough and you have a great insight of it then the couple of algorithm will do your work, and ofcourse the optimization techniques.

**Link to work**:[House price prediction](https://github.com/alokpadhi/House-Price-Prediction)

### Day 5: July 31, 2018

**Today's Progress**: Going deeper to the regression and learn more about regression, all thanks to the articles posted in analyticsvidhya.com 

**Thoughts**: Skill test on analyticsvidhya for linear regression and logistic regression. And the scores are as follows.
* Linear Regression: 26/30
* Logistic Regression: 24/30

**Due to Health issues I was unable to hold the challenge for 2days.

### Day 6: August 3, 2018

**Today's Progress**: Tried SVR with Grid search and Randomized search for house price prediction problem.

**Thoughts**: After using SVR with Grid search, the results were not better than Random forest, will look more into it and will find what I can do.

### Day 7: August 4, 2018

**Today's Progress**: Spent time on Google colabrotary, and floydhub setup

**Thoughts**: Tried floydhub cloud service, and use it for my basic house price machine learning model but I dont find it much faster. Though I used the free service for now.

### Day 8: August 5, 2018

**Today's Progress**: A day on reading Chapter 6 of ISLR, subset selection, stepwise selection method and choosing optimal method.

**Thoughts**: Before approaching a non linear way we should check all possibilites with the linear so this is what I learned, selecting important features for a linear model is very important.

### Day 9: August 6, 2018

**Today's Progress**: started working on a new project, classification of hand written digits using MNIST dataset.

**Thoughts**: So today I thougt before working on multiclass, I should work on single digit means a binary classfication. so far so good, confusion matrix as performance measure. Mention that to measure confusion matrix i used kfold crossvalidation.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 10: August 7, 2018

**Today's Progress**: Worked on binary classification for MNIST, did some more performance measure. 

**Thoughts**: Binary classification of digit recognition is about to finish, but before that I want to spend a little more time on exploring and visualizing data, and then will approach for multilabel classification task.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 11: August 8, 2018

**Today's Progress**: More time spent on confusion matrix, precision-recall trade off

**Thoughts**: almost 2-3 days on confusion matrix and still hang on it. The matrix is so simple but the  info that need to obtain is bit trickt. You need to figure out when precision is important and where recall. obviously you can opy for f1 score but those two are really need to be taken care off.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 12: August 9, 2018

**Today's Progress**: precision-recall tradeoff continues, plot precision-recall-threshold plot

**Thoughts**: so it's spending time on project, try to understand what each line of code provides, setting a threshold for precision and recall is really challenging, to get a better precision value. lowering the recall is not a good idea at all. That's where ROC may be a good approach let's see what happens.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 13: August 10, 2018

**Today's Progress**: completed work on the binary classification for a single digit in the MNIST data set

**Thoughts**: Didn't try so many algorithm, one linear algorithm and one random forest just to get some insights about how the data behaves by performing different algorithms on a single digit.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 14: August 11, 2018

**Today's Progress**: started work on multi class classification task on MNIST data set

**Thoughts**: Tried out onevsall method to get started, and the algorithm I used is stochastic gradient descent method.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 15: August 12, 2018

**Today's Progress**: Multi class classification task continued and worked on error analysis

**Thoughts**: using confusion matrix, analysing the error for multiclass problem is bit tricky as the confusion matrix array barely makes any sense. so it's better to plot the matrix and a visual insight clears a lot about the classifier.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 16: August 13, 2018

**Today's Progress**: Partial completed the multiclass MNIST task. more work on error analysis 

**Thoughts**: after look for the whole scene, sometimes analyising the single classes are really helpful, and after that you can imporove the classifier more.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 17: August 14, 2018

**Today's Progress**: simple Linear Regression with OLS method 

**Thoughts**: Linear regression is one of those algorithm that every peple get started with, linear regression is typically reminds of straight line. where the intercept and coefficent terms were also present though it's just about the simple linear regression.


### Day 18: August 15, 2018

**Today's Progress**: One more step to linear regression with gradient descent

**Thoughts**: Normalization is good but not enough to run a linear regression in all cases that's where gradient descent comes with it's 3 variants. Batch, stochastic, mini-batch gradient descent. All 3 methods are really good. where Batch GD ensure to give you a optimal point but takes bit long time when the training examples is large. but other two are doesn't bother about the size of training examples, but one bad thing about these is they gives you a good minimal point but the optimal point. But this can be also avoided with some tweaks with the necessary parameters. 

### Day19: August 16, 2018

**Today's Progress**: Ploynomial regression, Regularization(Ridge, Lasso, elastic net)

**Thoughts**: polynomial regression model implementation. Worked on regularization ridge, lasso regression, elastic net. Then comes logistic regression. Practical implementation was really important for these. More to learn and let your hands dirty on #Coding

### Day20: August 17, 2018

**Today's Progress**: machine learning time logitstic regression, softmax regression and svm  
**Thoughts**: A lot of time on machine learning only. logistic, softmax done. svm on andrew ng courese done. ready to do some more practical work using svm.


### Day21: August 18, 2018

**Today's Progress**: svm on udacity. and naive bayes on udacity too

**Thoughts**: naive bayes completed on udacity with a text classification using naive bayes with a 0.97 test accuracy.

### Day22: August 19, 2018

**Today's Progress**: svm section completed on udacity intro to ml 

**Thoughts**: Success fully implemented svm on the mini project and the accuracy is: 0.98

### Day23: August 20, 2018

**Today's Progress**: Played with iris data set , Taken analytics vidhya svm skill test

**Thoughts**: Exploring and learning svm, so tried svm with the iris data set as this one of the basic classification task. and through this it really helps to understand what svm doing with the data.
**Analytics Vidhya SVM Skill test score: 23/25

### Day24: August 21, 2018

**Today's Progress**: Decision tree started with iris data set and also decision tree on udacity's intro to ml

**Thoughts**: so its decision tree applied on iris data set. graphviz helps to draw a decision tree using sklearn. understand different parameters generated at each node like samples, values, gini. though i did not tried with entropy criteria as i feel gini is well enough. then started the decision tree section on udacity.

### Day25: August 22, 2018

**Today's Progress**: Decision tree completed on udacity, overfitting and bias-variance dilema

**Thoughts**: So with decision tree algorithm I got 97% accuracy on the terrain data. well not good than svm but I think with tweaking some hyper para meters I can predict a better score. like reducing max_feature, increasing min_samples_leaf etc. but still 97% accuracy not bad though.

### Day26: August 23, 2018

**Today's Progress**: started with ensemble learning, bagging and random forest and Choose your own algorithm on Udacity

**Thoughts**: well to test all these algorithm what I have learned, I used make_moon dataset from sklearn. They give fairly good understanding with VotingClassifier, BaggingClassifier and RandomForestClassifier. Then move to udacity to pick one algorithm of your own. I choose RandomForest and applied it to terrain data and got 92% accuracy and sadly I failed to beat the self driving score of 93.6%. I tried with tweaking some of hyper parameters but not getting good score than 92%. Also my oob_score was about 96%. but will try adaboost to see how it predicts.




