# 100 Days Of ML Code - Log

### Day 1: July 26, 2018

**Today's Progress**: Housing price prediction problem, spent a lot of time on Data and finally able to train model

**Thoughts**: working on data is really interesting, tried linear regresion, decission tree regressor, random forest with cross validation but still a good model need to find

**Link to work**:[House price prediction](https://github.com/alokpadhi/House-Price-Prediction)

### Day 2: July 27, 2018

**Today's Progress**: Worked on fine tuning the model using Grid search method

**Thoughts**: will use Random search and regularize to couter the overfiting problem

**Link to work**:[House price prediction](https://github.com/alokpadhi/House-Price-Prediction)

### Day 3,July 28, 2018

**Today's Progress**: Tried to learn about different optimization techniques

**Thoughts**: Just a learning phase, using different optimization and to know how and where to use them is really great

### Day 4: July 30, 2018

**Today's Progress**: some data visualization, and testing on different algorithm and finally fine tune them

**Thoughts**: when you're working on a machine learning project it is really necessary to work on data very very well, if your data is good enough and you have a great insight of it then the couple of algorithm will do your work, and ofcourse the optimization techniques.

**Link to work**:[House price prediction](https://github.com/alokpadhi/House-Price-Prediction)

### Day 5: July 31, 2018

**Today's Progress**: Going deeper to the regression and learn more about regression, all thanks to the articles posted in analyticsvidhya.com 

**Thoughts**: Skill test on analyticsvidhya for linear regression and logistic regression. And the scores are as follows.
* Linear Regression: 26/30
* Logistic Regression: 24/30

**Due to Health issues I was unable to hold the challenge for 2days.

### Day 6: August 3, 2018

**Today's Progress**: Tried SVR with Grid search and Randomized search for house price prediction problem.

**Thoughts**: After using SVR with Grid search, the results were not better than Random forest, will look more into it and will find what I can do.

### Day 7: August 4, 2018

**Today's Progress**: Spent time on Google colabrotary, and floydhub setup

**Thoughts**: Tried floydhub cloud service, and use it for my basic house price machine learning model but I dont find it much faster. Though I used the free service for now.

### Day 8: August 5, 2018

**Today's Progress**: A day on reading Chapter 6 of ISLR, subset selection, stepwise selection method and choosing optimal method.

**Thoughts**: Before approaching a non linear way we should check all possibilites with the linear so this is what I learned, selecting important features for a linear model is very important.

### Day 9: August 6, 2018

**Today's Progress**: started working on a new project, classification of hand written digits using MNIST dataset.

**Thoughts**: So today I thougt before working on multiclass, I should work on single digit means a binary classfication. so far so good, confusion matrix as performance measure. Mention that to measure confusion matrix i used kfold crossvalidation.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 10: August 7, 2018

**Today's Progress**: Worked on binary classification for MNIST, did some more performance measure. 

**Thoughts**: Binary classification of digit recognition is about to finish, but before that I want to spend a little more time on exploring and visualizing data, and then will approach for multilabel classification task.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 11: August 8, 2018

**Today's Progress**: More time spent on confusion matrix, precision-recall trade off

**Thoughts**: almost 2-3 days on confusion matrix and still hang on it. The matrix is so simple but the  info that need to obtain is bit trickt. You need to figure out when precision is important and where recall. obviously you can opy for f1 score but those two are really need to be taken care off.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 12: August 9, 2018

**Today's Progress**: precision-recall tradeoff continues, plot precision-recall-threshold plot

**Thoughts**: so it's spending time on project, try to understand what each line of code provides, setting a threshold for precision and recall is really challenging, to get a better precision value. lowering the recall is not a good idea at all. That's where ROC may be a good approach let's see what happens.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 13: August 10, 2018

**Today's Progress**: completed work on the binary classification for a single digit in the MNIST data set

**Thoughts**: Didn't try so many algorithm, one linear algorithm and one random forest just to get some insights about how the data behaves by performing different algorithms on a single digit.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 14: August 11, 2018

**Today's Progress**: started work on multi class classification task on MNIST data set

**Thoughts**: Tried out onevsall method to get started, and the algorithm I used is stochastic gradient descent method.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 15: August 12, 2018

**Today's Progress**: Multi class classification task continued and worked on error analysis

**Thoughts**: using confusion matrix, analysing the error for multiclass problem is bit tricky as the confusion matrix array barely makes any sense. so it's better to plot the matrix and a visual insight clears a lot about the classifier.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 16: August 13, 2018

**Today's Progress**: Partial completed the multiclass MNIST task. more work on error analysis 

**Thoughts**: after look for the whole scene, sometimes analyising the single classes are really helpful, and after that you can imporove the classifier more.

**Link to work**: [MNIST Classifier](https://github.com/alokpadhi/MNIST-Digit-Classfier.git)

### Day 17: August 14, 2018

**Today's Progress**: simple Linear Regression with OLS method 

**Thoughts**: Linear regression is one of those algorithm that every peple get started with, linear regression is typically reminds of straight line. where the intercept and coefficent terms were also present though it's just about the simple linear regression.


### Day 18: August 15, 2018

**Today's Progress**: One more step to linear regression with gradient descent

**Thoughts**: Normalization is good but not enough to run a linear regression in all cases that's where gradient descent comes with it's 3 variants. Batch, stochastic, mini-batch gradient descent. All 3 methods are really good. where Batch GD ensure to give you a optimal point but takes bit long time when the training examples is large. but other two are doesn't bother about the size of training examples, but one bad thing about these is they gives you a good minimal point but the optimal point. But this can be also avoided with some tweaks with the necessary parameters. 

### Day19: August 16, 2018

**Today's Progress**: Ploynomial regression, Regularization(Ridge, Lasso, elastic net)

**Thoughts**: polynomial regression model implementation. Worked on regularization ridge, lasso regression, elastic net. Then comes logistic regression. Practical implementation was really important for these. More to learn and let your hands dirty on #Coding

### Day20: August 17, 2018

**Today's Progress**: machine learning time logitstic regression, softmax regression and svm  
**Thoughts**: A lot of time on machine learning only. logistic, softmax done. svm on andrew ng courese done. ready to do some more practical work using svm.


### Day21: August 18, 2018

**Today's Progress**: svm on udacity. and naive bayes on udacity too

**Thoughts**: naive bayes completed on udacity with a text classification using naive bayes with a 0.97 test accuracy.

### Day22: August 19, 2018

**Today's Progress**: svm section completed on udacity intro to ml 

**Thoughts**: Success fully implemented svm on the mini project and the accuracy is: 0.98

### Day23: August 20, 2018

**Today's Progress**: Played with iris data set , Taken analytics vidhya svm skill test

**Thoughts**: Exploring and learning svm, so tried svm with the iris data set as this one of the basic classification task. and through this it really helps to understand what svm doing with the data.

**Analytics Vidhya SVM Skill test score: 23/25**

### Day24: August 21, 2018

**Today's Progress**: Decision tree started with iris data set and also decision tree on udacity's intro to ml

**Thoughts**: so its decision tree applied on iris data set. graphviz helps to draw a decision tree using sklearn. understand different parameters generated at each node like samples, values, gini. though i did not tried with entropy criteria as i feel gini is well enough. then started the decision tree section on udacity.

### Day25: August 22, 2018

**Today's Progress**: Decision tree completed on udacity, overfitting and bias-variance dilema

**Thoughts**: So with decision tree algorithm I got 97% accuracy on the terrain data. well not good than svm but I think with tweaking some hyper para meters I can predict a better score. like reducing max_feature, increasing min_samples_leaf etc. but still 97% accuracy not bad though.

### Day26: August 23, 2018

**Today's Progress**: started with ensemble learning, bagging and random forest and Choose your own algorithm on Udacity

**Thoughts**: well to test all these algorithm what I have learned, I used make_moon dataset from sklearn. They give fairly good understanding with VotingClassifier, BaggingClassifier and RandomForestClassifier. Then move to udacity to pick one algorithm of your own. I choose RandomForest and applied it to terrain data and got 92% accuracy and sadly I failed to beat the self driving score of 93.6%. I tried with tweaking some of hyper parameters but not getting good score than 92%. Also my oob_score was about 96%. but will try adaboost to see how it predicts.

### Day27: August 24, 2018

**Today's Progress**:  Ensemble learning continues with Adaboost. and applied on the #udacity intro to ml mini project

**Thoughts**: Adaboost really wonderful. Its one of the best time when I learned Adaboost. What's so cool about is the way it improvise upon a weak learner. Found some similarity with neural nets. The way it updated weights I mean incresing and decreasing as per necessity. and one more thing is to there aren't many hyperparameters as in RandomForest. Finally adaboost on the terrain data of udacity. and guess what I got 93.2% accuracy score, whereas their accuracy score was 93.6%. Hmm I might try out to learn a car for self driving. But dissapointed as I failed to beat that score.

### Day28: August 26, 2018

**Today's Progress**:  Ensemble learning Finished. Gradient boosting learned. 
**Thoughts**: So this is the day where I learned almost important supervised learning algorithms. To end this Gradient boosting technique. not a heavy day of coding but still its something to implement.

### Day29: August 28, 2018

**Today's Progress**: Dimensinality reduction technique. projection and manifold. #udacity intro to ml datasets POI

**Thoughts**: PCA to reduce dimensions. and exploring interest info on the enron datasets. so called POI in the udacity intro to ml course. some basic intutive and some coding to answer question.

### Day30: August 29, 2018

**Today's Progress**:  So POI on enron data continues.

**Thoughts**: Exploring more information on the enron dataset. moreover go deeper into the problem and did a case study, which will helps to analyse the problem domain and interest of features and targets.

### Day31: August 30, 2018

**Today's Progress**:  Diggind enron dataset finished. move to regression part. Python coding: Numpy, list and package

**Thoughts**: Regression analysis on the enron dataset.Used Sum of squares and r2 score to analysis the prediction. then some python coding on nummpy. explored list part on datacamp.

### Day32: August 31, 2018

**Today's Progress**:  Improve r2 score on enron dataset. python coding: matplotlib, hist(), tick, scatter(), labels, customizing plots

**Thoughts**: As at a very stage using r2 on the enron provides really bad score. then working out on the outliers provides a better result. and with this regression section finished for the udacity intro to ml section. some visualization learned using matplotlib. scatter plot, histogram, and bit more customization.

### Day33: september 1, 2018

**Today's Progress**:  Pandas dataframe and series, outlier detection

**Thoughts**: as I was using outlier on regression but that does not yield a better scope of outlier insight. so learning more detailed about outlier. one simple algorithm to work on outlier. some times outliers matters so it depens on the problem. and look into the outliers either they providing some valuable info or nor. then some pandas series and dataframes. python dicionary.

### Day34: September 3, 2018

**Today's Progress**:  Looping over pandas dataframe and a random walk simulation

**Thoughts**: Learned how to looping over numpy and pandas efficiently. a emperor state building step reaching simulation. using random walks,(numpy random) a simulation to predict what is the probability that you will reach at 60 step after the dice has been rolled. so the distribution probabilty says: 78.4%

### Day35: September 4, 2018

**Today's Progress**:  basic intermediate python skills to make a adwords for a furniture company

**Thoughts**: another real-world project. create a set of keywords for an online furniture retailer. using basic python and a couple of pandas function helps to generate a set of keywords and save that into a csv file.

**Link to Work**:



### Day36: September 5, 2018

**Today's Progress**:  arguments on functions, lambda functions and error handling

**Thoughts**: python coding: default function argument, flexible arguments. map(), filter(), reduce() with lambda function. error handling &  finally, a data science work using all the above: #tweets in different languages using twitter data.

### Day37: September 6, 2018

**Today's Progress**:  Python iterators, zip(), list comrehension, case study on world bank data

**Thoughts**: Learned about python iterators, list comprehensions & generators deeply. divided large-scale datasets into chunks and preprocess on each chunk. finally using all these worked on world bank data. python generators for streaming data, context manager for efficient file reading. iterators for streaming data

### Day38: September 8, 2018

**Today's Progress**: importing data using pandas, imported data from flat files

**Thoughts**: Learned about various ways of importing data from various sources. Reading a text flle using open(), read(), close(), readline(). worked with flat files using numpy and pandas. loadtxt(), genfromtxt(), recfromcsv(). imported using pandas functions read_csv() with some custom arguments.

### Day39: September 9, 2018

**Today's Progress**:  importing data extended with other file formats like pickle, excel spreadsheet, matlab files etc

**Thoughts**: started with imporing data from pickled files which allows converting objects to bytestream. then excel spreadsheet using pandas.cutomized spreadsheet import. imported SAS/Stata files. SAS7BDAT() used to import Statistical analysis system files and read_stata() to read stata files. then hierarchical data files using h5py. also worked with importing matlab data files.

### Day40: September 11, 2018

**Today's Progress**:  worked with relational databases file importing, imported from web by scraping using various python libraries.

**Thoughts**: collecting data from relational databases & store them as pandas data frame. import data from the web using urllib, requests. import files through HTTP requests using urllib & requests. then scrap web using BeautifulSoup, apis & json.

**Take Away**: sql_alchemy, create database engine and then use that engine to read database using sql query and then convert to pandas dataframe. urllib , requests. BeautifulSoup(), Soup.title, Soup.get_text(), Soup.findall(), json.

### Day41: September 12, 2018

**Today's Progress**:  Cleaning data started, EDA cycle, EDA visually, tidy data, melting, pivoting data, melting and parsing

**Thoughts**: cleaning data, common data problems, inconsistent column names, missing data, duplicate rows, processing columns, untidy data. exploratory data analysis with summary statistics with different pandas functions. then tried to analyze ED with different plots. Understands the concept of tidy data. melting and pivoting for the data.

**Take Away**: head(), tail(), .columns, .shape, info(), .value_counts() with dropna args. .describe() to see the summary stats. visual EDA with bar plots when there is discrete daa, histograms when continous data, box plots to visualize the basic summary stats. principles of tidy data. Melting to turn columns into rows and pivoting to turn unique values to separate columns. .split() and .get()

### Day42: September 13, 2018

**Today's Progress**:  concatenating data, globbing, merge data, prepare and clean data, regex, apply(), assert()

**Thoughts**: concatenating data, both row wise and column wise. finding and concatenating when a large numbers of data files are present with globbing. merging data frames with 1-1, 1-many/many-1, many-many. preapare and clean data with converting data types, using categorical data, converting string to numerical data. Used to regex to clean the strings. Dealt with complex cleaning with apply(), duplicate and missing data. finally testes with assert statement to check everyting is ok or not.

**Take away**: pd.concat() to concat more than one data files, finding relevant data files using glob, glob.glob() to match and concat. merging data files with pd.merge(), convert data types with .astype(), to_numeric() to convert data to numeric type, used re module to match patterns in string data. .apply(), .drop_duplicates() to drop duplicate rows. dealt with missing data, check to drop with .dropna() and to fill with .fillna(), either with some predefined value or with some statistics like meadian(), mean(). assert 

### Day43: September 14, 2018

**Today's Progress**:  DataFrame, series, broadcasting, inspect dataframe, plotting, EDA, statstical EDA, separate populations.

**Thoughts** and **Take Away**: 
* Slicing with iloc[], loc[]
* head(), tail(), info()
* Series
* Building dataframes from scratch
  * pd.DataFrame(dict)
  * used zip() to zipping two lists, one with list columns and list labels
  * then convert that zipped object to dictionary 
  * finally convert the dict to dataframe
* Broadcasting
* importing csv files
  * define column names to csv file with header=None, names=[]
  * missing values replaced with Nan with na_values args in read_csv()
  * parse dates with parse_dates args in read_csv()
  * info() to inspect data frame columns and types.
  * to_csv() and to_excel() to write into csv and excel files
* plotting with pandas
  * plot()
  * scaling with xscale and yscale
  * customize plots with color, style, legen args
  * savefig()
  * title()
  * subplots args in plot()
* Visual EDA
  * scatter plot
  * box plot
  * histogram
    * PDF to get PDF normed should True 
    * CDF to get CDF normed and cumulative both are True
* Statistical EDA
  * describe()
    * counts
    * mean
    * standard deviation
    * range
    * median
  * medians and quantile
    * inter quantile range
* separating populations
  * filtering by category

### Day44: September 15, 2018

**Today's Progress**:  Time series data in pandas

**Thoughts and Take Away**:
* Indexing time series
  * read_csv with parse_dates, string to datetime
  * index_col to Date column
* selecting datetime with loc
* slicing with loc[]
* convert string to datatime using to_datetime()
* reindexing
* fill missing value with forward fill and backward fill args
* Resampling
  * DownSampling
  * UpSampling
  * .resample()
  * .rolling() to smoothout short time fluctuations in time series
* manipulatig time series
  * string method with df[].str.str_method
  * .dt for datetime methods
  * converting timezone with dt.tz_convert()
  * interpolate missing data
* visualize time series 
  * line plots
  * style formating with color, marker, line type
  * area plots



### Day45: September 17, 2018

**Today's Progress**: Case study on whether data

**Thoughts**: whether data case study brings more practice for data analysis, both statistically and visually. some time series functionality also.

### Day46: September 18, 2018

**Today's Progress**: Case study on apple stock data

**Thoughts**: so a proper time series data sets, used to analyse stock price of apple. resampling, rolling, manipulating the data frame time series using pandas and obtain some useful information.

### Day47: September 20, 2018

**Today's Progress**:  Manipulating with pandas dataframe

**Thoughts**: 
* Indexing DataFrames
 * indexing using square brackets
 * using column and row
 * using loc and iloc
* Slicing dataframes
 * slicing rows and columns
 * slicing in reverse order
* Filtering dataframe
 * creating boolean series
 * filtering in presence of null values
  * isnull(), notnull()
* Transforming dataframes
 * vectorized methods
 * transformation
 * apply() and map()
* Index objects and labeled data
 * index name 
 * setting index with read_csv
* Hierarchical Indexing
 * multiindex
 * sorting index
 * slicing hierarchical index
 * filtering with fancy index
 * slice()
### Day48: September 21, 2018

**Today's Progress**:  Pivoting dataframes, catrogory group, titanic survival case study 

**Thoughts**:
* Pivoting DataFrames
 * reshaping 
 * stacking and unstacking
 * melting
 * pivot tables
* Catregoricals and groupby
 * groupby()
 * groupby() and aggfunc()
 * groupby() and transform()
 * splittting
* Case Study : Titanic Survival Problem

### Day49: September 22, 2018

**Today's Progress**:  Reading data files, indexing, appending and concatenating

**Thoughts**: 
* Reading multiple data files
 * read_csv()
 * using loop()
 * using comprhensions
 * using glob
* Reindexing dataframes
 * reindex()
 * sort_index()
 * sort_values()
* Appending and concatenating
 * append()
 * concat()
 * reset_index()
 * concatenation, keys and multiindex

### Day50: September 23, 2018

**Today's Progress**:  Join, merging dataframes, olympic case study

**Thoughts**:
* Outer join and inner join
 * stacking horizontally
 * stacking vertically
 * joins
 * .innerjoin
 * concatenate and innerjoin
 * concatenate and outerjoin
* Merging DataFrames
 * merge()
 * merging multiple columns
 * merge using suffix()
* Join dataframes
 * merge with left join
 * merge with right join
 * outerjoin
* case study: Olympic game 

### Day51: September 24, 2018

**Today's Progress**: SQL started, select columns,filtering results, null values

**Thoughts**: 
* selecting columns
 * SELECT
 * SELECT DISTINCT
 * COUNT()
* Filtering results
 * WHERE clause
 * WHERE AND OR
 * BETWEEN
 * WHERE IN
* Finding and treating null values
 * NULL
 * count missing values with COUNT() and NULL
* searching in data
 * LIKE to search a pattern
 * wild cards
 * NOT LIKE

### Day52: September 25, 2018

**Today's Progress**:  PostgreSQL aggregate functions, order by, group by, having


**Thoughts**:
* Aggregate functions
 * AVG, MAX, MIN, SUM
 * AS aliasing in aggregate columns
* ORDER BY
* GROUP BY
* HAVING

### Day53: September 26, 2018

**Today's Progress**:  worked with databased with python, SQLAlchemy, database connection, reflection, sql querying, filtering and targeting data

**Thoughts**: 
* SQLALchemy connection to database
* Reflection of tables with Table object and MetaData
* worked on sql queries like SELECT and WHERE
* insted of executing statement used MetaData and Table to help execute with SQLAlchemy
* Filtering and Targeting Data
 * execute with WHERE clause
 * in_(), like_(), between_(), and_(), any_(), or_(), not_()
 * ordering with order_by() and desc()
* Counting summing and grouping
 * Groupby with group_by
 * aliasing with .label() method
 * counting distinct values with .distinct()

### Day54: September 27, 2018

**Today's Progress**:  Visualizing, CASE staement, Joining, hierarchical table, large results sets, creating database, inserting data,update data and remove data from table

**Thoughts**:
* convert the Metadata table into data frame
* plotting the dataframe
* CASE statement
* automatic JOIN
* join with join()
* Hierarchical Tables
 * alias() to refeer a table with two unique names
 * self refering and join
* dealing with larger result sets
* create database with SQLAlchemy
* insert data into table
* update data into table
* finally remove data and drop table
### Day55: September 29, 2018

**Today's Progress**: worked with data science projects exploring 67 years of LEGO and evolution of LINUX

**Thoughts**:
 Both project works present in github repo [Real World Data Science Projects] 
(https://github.com/alokpadhi/Real-World-Data-Science-Projects)


### Day56: September 30, 2018

**Today's Progress**:  Today's project work is analyse the volatility of bitcoin and you should invest on it or not

**Thoughts**:
The project details present in the repo [Real World Data Science Projects](https://github.com/alokpadhi/Real-World-Data-Science-Projects)
This project is lieterally love by me, one of the enjoyable projects I have made.
 





